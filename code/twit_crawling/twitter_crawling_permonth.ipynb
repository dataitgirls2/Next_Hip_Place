{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특정단어(ex. 연남동)가 언급된 트위터 개수 **한 달씩** 크롤링하기\n",
    "\n",
    "<br>\n",
    "참고 : [왕형준님 블로그](https://medium.com/@whj2013123218/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-twitter-%ED%81%AC%EB%A1%A4%EB%A7%81-576f7b098daf) 갓형준님 감사합니다\n",
    "\n",
    "중복되는 코드를 없앴고 연도별 데이터를 보는 우리 프로젝트에 맞게 코드를 조금 수정했습니다.\n",
    "\n",
    "#### 먼저 코드를 실행시키기 위해서는\n",
    "\n",
    "1) Firefox 브라우저가 깔려있어야 합니다.\n",
    "\n",
    "2) geckodriver.exe가 현재 디렉토리에 있는지 확인해주세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total_twitter_permonth 함수 설명\n",
    "#### 트위터에서 제공하는 기존 twitter API의 문제점\n",
    " 무료 API는 최근 7일치 트위터밖에 가져오지 못한다. <br>\n",
    " 우리가 보려는 것은 2009년부터 현재까지 데이터이므로 twitter에서 제공하는 무료 API는 사용할 수 없었다.<br>\n",
    " 과거 데이터를 가져오려면 상당한 액수를 지불해야 한다. (우리 프로젝트라면 $399 짜리를 써야 했다...)\n",
    "<br>\n",
    "####  그래서 만든것이 total_twitter_permonth함수\n",
    "날짜, 지역(동), 트윗 수, 하트 수, 연도를 담아 데이터프레임으로 반환<br>\n",
    "트위터에서 특정 기간동안 특정 단어를 포함한 트윗들을 검색<br>\n",
    "스크롤을 내려 많은 트윗을 계속 로딩해 와야 하기 때문에 selenium으로 동적 크롤링을 했다.<br>\n",
    "chrome은 느리다는 말이 있어서 firefox를 이용했고 geckodriver로 크롤링 시행했다<br>\n",
    "#### 개발하면서 생긴 문제점들 \n",
    "1. 트위터 크롤링 시 로딩 오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"js-stream-whale-end stream-whale-end stream-placeholder centered-placeholder\">\n",
       "  <div class=\"stream-end-inner\">\n",
       "    <h2 class=\"title\">로딩하는데 시간이 지연되고 있습니다.</h2>\n",
       "    <p>\n",
       "      트위터의 트래픽이 폭주했거나 일시적인 문제가 발생했을 수 있습니다. <a role=\"button\" href=\"#\" class=\"try-again-after-whale\">다시 시도</a>하거나 <a target=\"_blank\" href=\"http://status.twitter.com\" rel=\"noopener\">트위터 상태 페이지</a>를 방문하여 자세한 내용을 확인해 보세요.\n",
       "    </p>\n",
       "  </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div class=\"js-stream-whale-end stream-whale-end stream-placeholder centered-placeholder\">\n",
    "  <div class=\"stream-end-inner\">\n",
    "    <h2 class=\"title\">로딩하는데 시간이 지연되고 있습니다.</h2>\n",
    "    <p>\n",
    "      트위터의 트래픽이 폭주했거나 일시적인 문제가 발생했을 수 있습니다. <a role=\"button\" href=\"#\" class=\"try-again-after-whale\">다시 시도</a>하거나 <a target=\"_blank\" href=\"http://status.twitter.com\" rel=\"noopener\">트위터 상태 페이지</a>를 방문하여 자세한 내용을 확인해 보세요.\n",
    "    </p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   해결방안: 위 문구가 나타나면 **트윗수(Frequency)**를 **-1**로 저장하도록 함\n",
    "\n",
    "2. 다음 날짜로 넘어갈 때 오류가 나서 datetime.timedelta대신 relativedelta 라이브러리 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import  FirefoxBinary\n",
    "from selenium.webdriver.common.desired_capabilities import  DesiredCapabilities\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_twitter_permonth(keyword,startyear,endyear):\n",
    "    binary=FirefoxBinary('C:/Program Files/Mozilla Firefox/firefox.exe')\n",
    "    browser=webdriver.Firefox(executable_path='geckodriver.exe',firefox_binary=binary)\n",
    "    \n",
    "    df = pd.DataFrame(data={'Date':[],'Frequency':[],'Heart':[],'Year':[],'Dong':[]})\n",
    "    \n",
    "    for loc in keyword:\n",
    "        print(loc)\n",
    "        for y in range(startyear,endyear+1):\n",
    "            startdate = dt.date(year=y,month=1,day=1)\n",
    "            untildate = dt.date(year=y,month=2,day=1)\n",
    "\n",
    "            if y == 2018 :\n",
    "                enddate = dt.date(year=y,month=9,day=1)\n",
    "            else :\n",
    "                enddate = dt.date(year=y+1,month=1,day=1)\n",
    "\n",
    "            totalfreq=[]\n",
    "            while not enddate == startdate:\n",
    "                url='https://twitter.com/search?&q=\"'+str(loc)+'\"%20since%3A'+str(startdate)+'%20until%3A'+str(untildate)+'&src=typd'\n",
    "                browser.get(url)\n",
    "\n",
    "                lastHeight = browser.execute_script(\"return document.body.scrollHeight\")  # 스크롤 내리기 전 위치\n",
    "                while True:\n",
    "                    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # 스크롤을 맨 아래까지 내림\n",
    "                    time.sleep(4)  # 로딩하는 시간때문에 sleep을 걸어줍니다.\n",
    "\n",
    "                    newHeight = browser.execute_script(\"return document.body.scrollHeight\")  # 현재 스크롤 위치\n",
    "\n",
    "                    if newHeight == lastHeight:  # 더이상 로딩될 트윗이 없다면\n",
    "                        html = browser.page_source\n",
    "                        soup = BeautifulSoup(html,'html.parser')\n",
    "                                                                  \n",
    "                        tweets = soup.find_all(\"p\", {\"class\": \"TweetTextSize\"})\n",
    "                        wordfreq = len(tweets)  # 페이지에 있는 트윗개수\n",
    "\n",
    "                        heartfreq = 0\n",
    "                        hearts = soup.find_all(\"span\",text = re.compile(\"\\d 마음\"))\n",
    "                        for hf in hearts:\n",
    "                            heartfreq += int(hf.text[:-3].replace(',',''))\n",
    "\n",
    "                        if(len(soup.find_all(\"h2\",text = re.compile(\"로딩하는데 시간이 지연되고 있습니다.\"))) == 1):\n",
    "                            wordfreq = -1\n",
    "                            \n",
    "                        dailyfreq = {'Date':startdate,'Frequency':wordfreq,'Heart':heartfreq,'Year':startdate.year,'Dong':loc}\n",
    "                        totalfreq.append(dailyfreq)\n",
    "\n",
    "                        # 다음 작업을 위해 하루 늘려줌                  \n",
    "                        startdate = untildate\n",
    "                        untildate += relativedelta(months=1)\n",
    "                        # 초기화\n",
    "                        wordfreq = 0\n",
    "                        dailyfreq = {}\n",
    "                        break\n",
    "                    lastHeight = newHeight\n",
    "\n",
    "            df = pd.concat([df,pd.DataFrame(totalfreq)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 본격 서울시 467개동 크롤링\n",
    "\n",
    "기존 공시지가 데이터에서 서울시 법정동 이름만 추출<br>\n",
    "XX1가동 XX1동 과 같은 경우는 해당 트윗이 많이 없을것이라 예상하고 XX동으로 전처리를 시켜주었다.\n",
    "총 358개동이 추출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "land = pd.read_csv('Report.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dong_list = []\n",
    "for d in land['법정동'].unique()[3:]:\n",
    "    d = re.sub('\\d가','',d)\n",
    "    d = re.sub('\\d동','동',d)\n",
    "    dong_list.append(d)\n",
    "dong_list = list(set(dong_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dong_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "염곡동\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내발산동\n",
      "관수동\n",
      "번동\n",
      "교북동\n",
      "이태원동\n",
      "여의도동\n",
      "보문동\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,71):\n",
    "    df = total_twitter_permonth(dong_list[5*n:5*(n+1)],2009,2017)\n",
    "    df.to_csv('twit_seoul/seoul_dong_'+str(n)+'.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이태원동\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여의도동\n",
      "보문동\n"
     ]
    }
   ],
   "source": [
    "df = total_twitter_permonth(dong_list[355:],2009,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_72.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연남동\n"
     ]
    }
   ],
   "source": [
    "df = total_twitter_permonth(['연남동'],2009,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_yn.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = total_twitter_permonth(['익선동'],2009,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_is.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "후암동\n"
     ]
    }
   ],
   "source": [
    "df = total_twitter_permonth(['후암동'],2009,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_ha.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = total_twitter_permonth(['성수동'],2015,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_ss.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "봉천동\n"
     ]
    }
   ],
   "source": [
    "df = total_twitter_permonth(['봉천동'],2009,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_bc.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "망원동\n"
     ]
    }
   ],
   "source": [
    "df = total_twitter_permonth(['망원동'],2009,2017)\n",
    "df.to_csv('twit_seoul/seoul_dong_mw.csv',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
